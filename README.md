# Ограничение генерации LLM на уровне логитов

Токен-маскинг и логит-редактирование для запрета нежелательных слов

> Research Proposal для моей дипломной работы

## Теория

> Прежде чем писать работу, нужно найти источники. Прежде чем искать источники, нужно понять о чём я буду писать

### Что такое логиты

**Логиты** - неотнормированные вероятности токенов, выход из последнего слоя нейросети перед тем, как выбрать следующий токен.

По сути логит - оценка *желательности* токена в текущем контексте.

Логит - **вектор чисел**, в котором каждому токену из словаря модели вроде `the`, `cat`, `##ing`, `<|endoftext|>`, соответствует одно число.

Например:

```python
# длина списка = размер словаря модели, обычно более 50к
logits = [2.5, -1.2, .0, 4.7, ...]
```

#### Логиты и вероятность

Логиты проходят через **softmax**-функцию, которая превращает их в вероятности:

```python
probs = softmax(logits)
```

Чем выше логит, тем выше вероятность токены. Логиты определяют **распределение вероятностей** для следующего токена

Рассмотрим на примере:

```python
logits = torch.tensor([0.5, 2.0, 1.0])
probs = torch.softmax(logits, dim=-1)

# probs = [0.14, 0.63, 0.23]
```

Токен с логитом `2.0` будет выбран с вероятностью 63%, а `0.5` - с 14%

#### Логиты в генерации

Генерация LLM - это выбор следующего токена на основе логитов:

1. Модель прочитала строку `I want to`
2. Выдала логиты для всех токенов
3. Из логитов сделала вероятности с помощью *softmax*
4. По вероятностям либо выбирает случайно, либо берёт самый вероятный вариант

Если мы устанавливает логит в `-inf`, то фактически **запрещаем токен** - его вероятность становится $0$ => он не появится в генерации

### Откуда модель берёт логиты?

Модель вычисляет логиты на основе входного текста, через свои внутренние слои.

Например, я подаю на вход:

```python
 input_ids = tokenizer('Hello, I love you!', return_tensors='pt').input_ids

>>> input_ids
tensor([[15496, 11, 314, 1842, 345, 0]])
 ```

Каждое число - `token_id` - проходит через **встраивание** *(embedding layer)*, специальную таблицу которая сопоставляет каждому токену вектор

```text
[15496] > [.2, -.4, ..., 1.0] # вектор эмбеддинга
```

Так модель получает последовательность векторов:

```text
X = [e₁, e₂, e₃, e₄, e₅]
```

Модель состоит из **трансформерной архитектуры**: механизмы внимания *(self-attention)*, нормализации и линейные преобразования

```text
X > layer_1 > layer_2 > layer_N > H
```

На выходе тензор `H` - векторы признаков для каждого токена во входной последовательности. Например, в таком случае:

```python
H = model.transformer('Hello, I love you') 

H.shape = [1, 5, 768]
```

У кадого токена есть вектор признаков из **768 чисел**: осмысленное представление токена в контексте всех остальных токенов

Для каждого токена нам нужен один логин. Допустим, словарь модели содержит $50 000$ токенов. Какая вероятность, что следующим токеном будет каждый из этих $50 000$?

Модель берёт последний вектор из H, соответствующий последнему токену `you`, и пропускает его через **линейный слой**:

$$
\text{logits} = h_{\text{last}} \times W^T
$$

- $\text{h_last}$ - вектор размера `hidden_size`, например $768$
- $W$ - матрица весов, например $[50 000, 768]$

Результат - `logits` = вектор $[50 000]$, по одному числу на каждый токен

У правильного следующего токена логит выше, чем у остальных - так настроена модель во время обучения. Модель сопоставляет каждый токен со всеми возможными словами.

### Как логиты контролируют генерацию?

Механизм генерации повторяется каждый шаг: модель получает `input_ids`, выдаёт `logits` для следующего токена, преобразует `logits` в `probabilities` через *softmax*, выбирает токен, добавляет к контексту и начинает заново.

Логит - по сути оценка привлекательности токена. Хотим запретить токен - делаем его непривлекательным.

Softmax математически выглядит так:

$$
P(t_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$

Если логит $z_i = - \infty$, то $e^{z_i} = 0$. Значит, токен $t_i$ получит нулевую вероятность, и его невозможно выбрать даже при сэмплинге

Мы можем запретить любой токен, присвоив ему логит $\infty$ перед *softmax*
